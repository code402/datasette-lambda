#!/bin/bash

set -euo pipefail
set -x

logfile=$(mktemp)
lambdazip=$(readlink -m package.zip)
db_paths=()
dbfiles=
cors=False
EXTERNAL=${EXTERNAL:-0}

# Per https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html,
# the total unzipped size of the deployment package, including layers, must be
# less than 250 MB. Datasette and its deps are about 30 MB, so that leaves 220 MB
# for embedded SQLite DBs.
deployment_size_limit=220000000

# Per the same doc, the zip file we upload needs to be less than 50 MB.
zip_size_limit=50000000

# Per the same doc, /tmp has a 512 MB limit.
tmp_size_limit=512000000

finish() {
  rm -f "${logfile}"
}
trap finish EXIT

stack_exists() {
  local stack_name
  stack_name=${1:?must provide stack name}
  aws cloudformation describe-stacks --stack-name "${stack_name}" &> /dev/null
}

has_s3patch() {
  which s3patch > /dev/null
}

s3cp() {
  from=${1:?must specify local file, e.g. file.zip}
  to=${2:?must specify remote file, e.g. s3://bucket/key}

  if has_s3patch; then
    s3patch -v cp "${from}" "${to}"
  else
    aws s3 cp "${from}" "${to}"
  fi
}

create_or_update_stack() {
  local stack_name
  local rv
  stack_name=${1:?must specify stack name}
  shift

  if ! stack_exists "${stack_name}"; then
    aws cloudformation create-stack --stack-name="${stack_name}" "$@"
    aws cloudformation wait stack-create-complete --stack-name="${stack_name}"
  else
    if aws cloudformation update-stack --stack-name="${stack_name}" "$@" |& tee "${logfile}"; then
      aws cloudformation wait stack-update-complete --stack-name="${stack_name}"
    else
      rv=$?
      # There must be a less janky way to detect this. When the template hasn't changed,
      # the update-stack command will fail because there are no updates to perform.
      # That's not what we'd like; so we check the output for this string. It'll
      # likely fail to detect this case in non-English locales, though.
      if grep --silent "No updates are to be performed" "${logfile}"; then
        return 0
      fi
      return "${rv}"
    fi
  fi
}

create_lambda_zip() {
  rm -f "${lambdazip}"
  pushd app
  zip --quiet -r "${lambdazip}" .
  popd
}

parse_opts() {
  local db_size=0 size file

  rm -f app/metadata.json
  rm -f app/config.txt

  while [ "$#" -gt 0 ]; do
    case "$1" in
      --config)
        shift
        config_value=${1:?error: --config requires config value argument}
        echo "${config_value}" >> app/config.txt
        ;;

      --cors)
        cors=True
        ;;

      -m|--metadata)
        shift
        file=${1:?error: --metadata requires file argument}
        if [ ! -e "${file}" ]; then
          echo "error: --metadata points to non-existent file ${file}" > /dev/stderr
          exit 2
        fi

        cp "${file}" app/metadata.json
        ;;
      *)
        db_paths+=("$1")
        ;;
    esac
    shift
  done

  if [ "${#db_paths[@]}" -eq 0 ]; then
    echo "error: must specify at least one database" > /dev/stderr
    exit 1
  fi

  # All the database files passed must exist.
  for db in "${db_paths[@]}"; do
    if [ ! -e "${db}" ]; then
      echo "error: cannot find database ${db}" > /dev/stderr
      exit 2
    fi
    # The aws-cli CloudFormation support is weird. If you pass commas,
    # it tries to interpret it as a List<String> and fails.
    # Lazy workaround is to separate using a non-comma.
    dbfiles="${dbfiles}${dbfiles:+@}$(basename "${db}")"
    size=$(stat --printf="%s" "${db}")
    db_size=$((db_size + size))
  done

  if [ "${db_size}" -gt "${tmp_size_limit}" ]; then
    echo "error: Lambda has a 512 MB limit, but total size of DBs is ${db_size}; cannot proceed" > /dev/stderr
    exit 1
  fi

  if [ "${EXTERNAL}" == "0" ] && [ "${db_size}" -gt "${deployment_size_limit}" ]; then
    echo "info: EXTERNAL=0 but total size of DBs is ${db_size}; falling back to EXTERNAL=1" > /dev/stderr
    EXTERNAL=1
  fi
}

main() {
  stack_name=${1:?must provide CloudFormation stack name}
  shift

  parse_opts "$@"

  create_lambda_zip

  if [ "${EXTERNAL}" == "0" ]; then
    for db in "${db_paths[@]}"; do
      dir=$(dirname "${db}")
      pushd "${dir}"
      zip --quiet "${lambdazip}" "$(basename "${db}")"
      popd
    done

    size=$(stat --printf="%s" "${lambdazip}")
    if [ "${size}" -gt "${zip_size_limit}" ]; then
      echo "info: EXTERNAL=0 but total size of zip is ${size}; falling back to EXTERNAL=1" > /dev/stderr
      EXTERNAL=1

      create_lambda_zip
    fi
  fi

  create_or_update_stack "${stack_name}" --template-body=file://stack-prereq.yaml
  bucket=$(aws cloudformation describe-stack-resources --stack-name "${stack_name}" --logical-resource-id S3Bucket --query StackResources[0].PhysicalResourceId --output text)

  s3cp "${lambdazip}" s3://"$bucket"/package.zip

  create_or_update_stack "${stack_name}-lambda" \
    --capabilities CAPABILITY_IAM \
    --parameters \
      ParameterKey=Bucket,ParameterValue="${bucket}" \
      ParameterKey=CORS,ParameterValue="${cors}" \
      ParameterKey=DbFiles,ParameterValue="${dbfiles}" \
    --template-body=file://stack-lambda.yaml

  if [ "${EXTERNAL}" == "1" ]; then
    for db in "${db_paths[@]}"; do
      s3cp "${db}" s3://"${bucket}"/"$(basename "${db}")"
    done
  fi

  lambda=$(aws cloudformation describe-stack-resources --stack-name "${stack_name}-lambda" --logical-resource-id LambdaFunction --query StackResources[0].PhysicalResourceId --output text)
  aws lambda update-function-code --function-name "$lambda" --s3-bucket "${bucket}" --s3-key package.zip

  distribution_id=$(aws cloudformation describe-stack-resources --stack-name "${stack_name}-lambda" --logical-resource-id CloudFrontDistribution --query StackResources[0].PhysicalResourceId --output text)
  aws cloudfront create-invalidation --distribution-id "${distribution_id}" --paths '/*'

  aws cloudformation describe-stacks --stack-name "${stack_name}"-lambda --query Stacks[0].Outputs
}

main "$@"
